{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582b68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, NoSuchFrameException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0db3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74aa80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cafe_crawling2():\n",
    "    description_state = False\n",
    "    try:\n",
    "        description_a_link = driver.find_element(By.CSS_SELECTOR, \"div.PIbes > div.O8qbU > div.vV_z_ > a > span.zPfVt\")\n",
    "        description_a_link.click()\n",
    "        description_state = True\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "\n",
    "    # 기본 카페 정보 크롤링\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    address = soup.find('span', class_='LDgIH').text if soup.find('span', class_='LDgIH') else \"\"\n",
    "    if not \"대구\" in address and not \"경산\" in address:\n",
    "        return None\n",
    "\n",
    "    cafe_name = soup.find('span', class_='Fc1rA').text\n",
    "    cafe_type = soup.find('span', class_='DJJvD').text\n",
    "    \n",
    "\n",
    "    image_div = soup.find('div', {'id': 'ibu_1'})\n",
    "    if image_div:\n",
    "        style = image_div.get('style')\n",
    "        background_image_url = re.search(r'url\\((.*?)\\)', style).group(1)\n",
    "    else:\n",
    "        background_image_url = \"\"\n",
    "    \n",
    "    # reviews_div = soup.find('div', class_='dAsGb')\n",
    "    # starring = float(reviews_div.find('span', class_='PXMot LXIwF').text.strip()[2:]) if reviews_div.find('span', class_='PXMot LXIwF') else 0\n",
    "    # if not reviews_div.find_all('span', class_='PXMot') or len(reviews_div.find_all('span', class_='PXMot')) < 3:\n",
    "    #     return None\n",
    "    # visitor_reviews = int(reviews_div.find_all('span', class_='PXMot')[1 if starring else 0].text.strip().split(\" \")[1].replace(\",\", \"\"))\n",
    "    # blog_reviews = int(reviews_div.find_all('span', class_='PXMot')[2 if starring else 1].text.strip().split(\" \")[1].replace(\",\", \"\"))\n",
    "    \n",
    "    try:\n",
    "        # 별점 요소 찾기\n",
    "        star_rating_element = driver.find_element(By.CSS_SELECTOR, \"div.dAsGb > span.PXMot.LXIwF\")\n",
    "        starring = float(star_rating_element.text.strip()[2:])\n",
    "    except NoSuchElementException:\n",
    "        # 별점 요소가 없는 경우\n",
    "        starring = 0\n",
    "\n",
    "    try:\n",
    "        # 방문자 리뷰 요소 찾기\n",
    "        visitor_review_element = driver.find_element(By.XPATH, \"//span[@class='PXMot']/a[contains(@href, '/review/visitor')]\")\n",
    "        visitor_reviews = int(visitor_review_element.text.strip().split(\" \")[1].replace(\",\", \"\"))\n",
    "    except NoSuchElementException:\n",
    "        # 방문자 리뷰 요소가 없는 경우\n",
    "        visitor_reviews = 0\n",
    "        \n",
    "    try:\n",
    "        # 블로그 리뷰 요소 찾기\n",
    "        blog_review_element = driver.find_element(By.XPATH, \"//span[@class='PXMot']/a[contains(@href, '/review/ugc')]\")\n",
    "        blog_reviews = int(blog_review_element.text.strip().split(\" \")[1].replace(\",\", \"\"))\n",
    "    except NoSuchElementException:\n",
    "        # 블로그 리뷰 요소가 없는 경우\n",
    "        blog_reviews = 0\n",
    "    \n",
    "    if visitor_reviews < 10:\n",
    "        return None\n",
    "    #PIbes > O8qbU > vV_z_ > zPfVt\n",
    "    # description = soup.find_all('span', class_='zPfVt')[1].text if len(soup.find_all('span', class_='zPfVt')) > 1 else \"\"\n",
    "    if description_state:\n",
    "        description = driver.find_element(By.CSS_SELECTOR, \"div.PIbes > div.O8qbU > div.vV_z_ > a > span.zPfVt\").text\n",
    "    else:\n",
    "        try:\n",
    "            description = driver.find_element(By.CSS_SELECTOR, \"div.PIbes > div.O8qbU > div.vV_z_ > div.xHaT3 >  span.zPfVt\").text\n",
    "        except:\n",
    "            description = \"\"\n",
    "    address = soup.find('span', class_='LDgIH').text if soup.find('span', class_='LDgIH') else \"\"\n",
    "    business_hours = soup.find('time').text if soup.find('time') else \"\" \n",
    "    phone_number = soup.find('span', class_='xlx7Q').text if soup.find('span', class_='xlx7Q') else \"\"\n",
    "\n",
    "    # 키워드로 넘어가기\n",
    "    keyword_tab_link = driver.find_element(By.XPATH, \"//div[@class='flicking-camera']/a[.//span[contains(text(), '리뷰')]]\")\n",
    "    keyword_tab_link.click()\n",
    "    \n",
    "    keyword_state = False\n",
    "    idx = 0\n",
    "    while not keyword_state and idx < 5:\n",
    "        try:\n",
    "            keyword_list_link = WebDriverWait(driver, 1).until(EC.presence_of_element_located((By.CLASS_NAME, \"Tvx37\")))\n",
    "            keyword_list_link.click()\n",
    "            keyword_state = True\n",
    "        except:\n",
    "            keyword_tab_link.click()\n",
    "            keyword_tab_link.click()\n",
    "            keyword_tab_link.click()\n",
    "            keyword_tab_link.click()\n",
    "        idx+=1\n",
    "    \n",
    "#     for _ in range(3):\n",
    "#         try:\n",
    "#             keyword_list_link = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"Tvx37\")))\n",
    "#             keyword_list_link.click()\n",
    "#             keyword_state = True\n",
    "#         except:\n",
    "#             keyword_tab_link.click()\n",
    "    \n",
    "    if not keyword_state:\n",
    "        return [cafe_name, cafe_type, starring, visitor_reviews, blog_reviews, address, business_hours, phone_number, \"\", description, \"\", \"\", \"\", background_image_url]\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            keyword_more_link = driver.find_element(By.CLASS_NAME, 'Tvx37')\n",
    "            keyword_more_link.click()\n",
    "        except Exception as ex:\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    keyword_number = int(driver.find_element(By.CSS_SELECTOR, \"div._Wmab > em\").text.replace(\",\", \"\"))\n",
    "\n",
    "    keywords = {}\n",
    "    ui_tags = soup.find('ui', class_='uNsI9')\n",
    "    li_tags = soup.find_all('li', class_='nbD78')\n",
    "\n",
    "    for li in li_tags:\n",
    "        # 첫 번째 숫자를 int 형으로 변환\n",
    "        text = li.find('span', class_='nWiXa').text.strip('\"')\n",
    "        numbers = re.findall(r'\\d+', li.find('span', class_='TwM9q').text)\n",
    "        number = int(numbers[0]) if numbers else None\n",
    "        keywords[text] = number\n",
    "    \n",
    "    return [cafe_name, cafe_type, starring, visitor_reviews, blog_reviews, address, business_hours, phone_number, \"\", description, \"\", keywords, keyword_number, background_image_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1450e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_search_list= [\n",
    "                   \"동성로\", \"대구 중앙로\", \"교동\", \"경북대병원역\",\n",
    "                    \"동대구역\", \"대구 평화시장\", \"안심역\",\n",
    "                    \"대구 광장코아\", \"만평역\", \"대구 서구청\",\n",
    "                    \"명덕역\", \"대구 앞산\",\n",
    "                    \"경북대 북문\", \"대구 복현오거리\", \"동천동 카페\", \"칠곡네거리\",\n",
    "                    \"범어네거리\", \"수성유원지\", \"시지\", \"지산\",\n",
    "                    \"두류공원\", \"용산\", \"월배\", \"계명대\"\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbf343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['읍천리382 본점', '카페', 4.51, 1467, 749, '대구 남구 안골길 17-1', '08:30에 영업 시작', '0507-1326-0382', '', '', '', '', '', '']\n",
      "['민스크', '카페', 4.39, 1396, 675, '대구 남구 현충로6길 9-2', '11:00에 영업 시작', '053-656-6292', '', '', '', {'커피가 맛있어요': 436, '인테리어가 멋져요': 300, '디저트가 맛있어요': 241, '주차하기 편해요': 236, '음료가 맛있어요': 224, '뷰가 좋아요': 181, '대화하기 좋아요': 163, '매장이 청결해요': 149, '친절해요': 136, '사진이 잘 나와요': 114, '특별한 메뉴가 있어요': 75, '좌석이 편해요': 63, '화장실이 깨끗해요': 39, '가성비가 좋아요': 31, '집중하기 좋아요': 24, '매장이 넓어요': 5, '야외 공간이 멋져요': 3, '컨셉이 독특해요': 2, '차분한 분위기에요': 1, '비싼 만큼 가치있어요': 1, '단체모임 하기 좋아요': 1}, 954, '\"https://search.pstatic.net/common/?autoRotate=true&type=w560_sharpen&src=https%3A%2F%2Fldb-phinf.pstatic.net%2F20200218_11%2F15820174470467G7a6_JPEG%2FsH1Nn2zjcJ0aXIQkXOeYhbX6.jpg\"']\n",
      "['브리에브레드', '베이커리', 0, 47, 84, '대구 남구 현충로7길 7 브리에브레드', '09:00에 영업 시작', '0507-1355-5421', '', '', '', '', '', '\"https://search.pstatic.net/common/?autoRotate=true&type=w560_sharpen&src=https%3A%2F%2Fldb-phinf.pstatic.net%2F20231001_86%2F1696146108886D30gM_JPEG%2FKakaoTalk_20230930_173838508_03.jpg\"']\n",
      "['아눅 앞산', '베이커리', 4.51, 2936, 2049, '대구 남구 앞산순환로 459 아눅 앞산', '10:00에 영업 시작', '0507-1422-1060', '', '', '', '', '', '\"https://search.pstatic.net/common/?autoRotate=true&type=w560_sharpen&src=https%3A%2F%2Fldb-phinf.pstatic.net%2F20220913_2%2F1663037598799HC44M_JPEG%2FKakaoTalk_20220913_115245058.jpg\"']\n"
     ]
    }
   ],
   "source": [
    "for cafe_name in [\"대구 앞산\"]:\n",
    "    try:\n",
    "        df = pd.DataFrame(columns=['cafe_name', \"cafe_type\", 'starring', 'visitor_review', 'blog_review', 'address', 'business_hours', 'phone_number', 'connection_site', 'description',\n",
    "                                    \"menu\", \"keyword\", 'keyword_number', 'image_link'])\n",
    "        # WebDriver 초기화\n",
    "        driver = webdriver.Chrome() # Chrome 드라이버 사용\n",
    "        driver.get(f\"https://map.naver.com/p/search/{cafe_name} 카페\") # 네이버 지도 접속\n",
    "\n",
    "        # 결과 로드 대기\n",
    "        time.sleep(3) # 페이지 로드를 위해 잠시 대기\n",
    "\n",
    "\n",
    "        while True:\n",
    "            # searchIframe으로 컨텍스트 전환 + div scroll 내리기\n",
    "            driver.switch_to.frame(\"searchIframe\")\n",
    "            scroll_div = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"_pcmap_list_scroll_container\")))\n",
    "            for _ in range(5):\n",
    "                driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scroll_div)\n",
    "                time.sleep(1)\n",
    "\n",
    "            # 목록을 담고 있는 ul 태그를 찾고, 그 안의 모든 li 태그를 가져옴\n",
    "            cafe_list = scroll_div.find_elements(By.TAG_NAME, \"li\")\n",
    "            cafe_size = len(cafe_list)\n",
    "\n",
    "\n",
    "            for item in cafe_list:\n",
    "                try:\n",
    "                    place_bluelink = item.find_element(By.CLASS_NAME, \"place_bluelink\")\n",
    "                    # 부모의 부모인 a 태그 찾기 및 클릭\n",
    "                    parent_a_tag = place_bluelink.find_element(By.XPATH, \"../..\")\n",
    "                    parent_a_tag.click()\n",
    "\n",
    "                    time.sleep(3)\n",
    "\n",
    "                    # 필요한 작업 수행 (예: 상세 정보 수집)\n",
    "                    driver.switch_to.default_content()\n",
    "                    driver.switch_to.frame(\"entryIframe\")\n",
    "                    cafe_row = cafe_crawling2()\n",
    "                    if cafe_row:\n",
    "                        df.loc[len(df)] = cafe_row\n",
    "                        print(cafe_row)\n",
    "\n",
    "                    # 다음 카페로 이동하기 전에 searchIframe으로 다시 컨텍스트 전\n",
    "                    driver.switch_to.default_content()\n",
    "                    driver.switch_to.frame(\"searchIframe\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass\n",
    "\n",
    "            next_a_link = driver.find_elements(By.CLASS_NAME, \"eUTV2\")[1]\n",
    "            aria_disabled = next_a_link.get_attribute(\"aria-disabled\")\n",
    "            # 'aria-disabled'가 'false'이면 a 태그 클릭\n",
    "            if aria_disabled == \"false\":\n",
    "                next_a_link.click()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # 원래 프레임으로 돌아감\n",
    "            driver.switch_to.default_content()\n",
    "    finally:\n",
    "            df.to_csv(f\"./new_cafe_data/{cafe_name}.csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2d12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
